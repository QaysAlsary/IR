{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading successful\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from text_processing.TextProcessor import TextProcessor\n",
    "from Tfidf.Tf_idf_Service import TfidfService\n",
    "from Ranking.ranking_service import RankingService\n",
    "\n",
    "# Instantiate services\n",
    "tfidf_service = TfidfService()\n",
    "textprocessor = TextProcessor()\n",
    "rankingservice = RankingService()\n",
    "\n",
    "# Load the saved vectorizer, TF-IDF matrix, and corpus\n",
    "cwd = pathlib.Path().cwd()\n",
    "clinical_tfidf_matrix_path = cwd / 'files' / 'clinicaltrials' / 'tfidf_matrix.pkl'\n",
    "lifestyle_tfidf_matrix_path = cwd / 'files' / 'lifestyle' / 'tfidf_matrix.pkl'\n",
    "clinical_corpus_key_path = cwd / 'files' / 'clinicaltrials' / 'corpuskey.json'\n",
    "lifestyle_corpus_key_path = cwd / 'files' / 'lifestyle' / 'corpuskey.json'\n",
    "lifestyle_folder = cwd / \"files\" / \"lifestyle/\"\n",
    "clinical_folder = cwd / \"files\" / \"clinicaltrials/\"\n",
    "tfidf_service.preload(lifestyle_folder, clinical_folder)\n",
    "\n",
    "with open(clinical_corpus_key_path, 'r') as f:\n",
    "    clinical_corpus_keys = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docsIdsSearch(query_text, corpus_keys, dataset):\n",
    "    processed_query = textprocessor.process_generic(query_text)\n",
    "    \n",
    "    query_vector = tfidf_service.vectorize_query_evaluation(processed_query, dataset)\n",
    "    \n",
    "    similarity_scores = tfidf_service.calculate_similarity(query_vector, dataset)\n",
    "    \n",
    "    sorted_ranks = rankingservice.rank_and_sort(similarity_scores)\n",
    "    return [corpus_keys[doc_idx] for doc_idx, _ in sorted_ranks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the qrel JSONL file for clinical trials\n",
    "def load_qrel_clinical(jsonl_file_path):\n",
    "    qrel_dict = defaultdict(dict)\n",
    "    real_relevant = defaultdict(list)\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line.strip())\n",
    "            query_id = entry[\"query_id\"]\n",
    "            docs = entry[\"docs\"]\n",
    "            for doc in docs:\n",
    "                doc_id = doc[\"doc_id\"]\n",
    "                relevance = doc[\"relevance\"]\n",
    "                qrel_dict[query_id][doc_id] = relevance\n",
    "                if relevance > 0:\n",
    "                    real_relevant[query_id].append(doc_id)\n",
    "    return qrel_dict, real_relevant\n",
    "\n",
    "# Paths to QREL files\n",
    "clinical_qrel_path = cwd / \"files\" / 'clinicaltrials' / \"qrel.jsonl\"\n",
    "\n",
    "# Load QREL data\n",
    "clinical_qrel_dict, real_relevant_clinical = load_qrel_clinical(clinical_qrel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for each clinical query...\n",
      "Clinical document retrieval complete.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents for each query using the search service\n",
    "retrieved_docs_clinical = {}\n",
    "print(\"Retrieving documents for each clinical query...\")\n",
    "dataset = \"clinicaltrials\"\n",
    "with open(clinical_qrel_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line.strip())\n",
    "        query_id = entry[\"query_id\"]\n",
    "        query_text = entry[\"text\"]\n",
    "        ret_docs = docsIdsSearch(query_text, clinical_corpus_keys, dataset)\n",
    "        retrieved_docs_clinical[query_id] = ret_docs\n",
    "print(\"Clinical document retrieval complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    def __init__(self, true_data, predictions):\n",
    "        self.true_data = {str(k): list(map(str, v)) for k, v in true_data.items()}\n",
    "        self.predictions = {str(k): list(map(str, v)) for k, v in predictions.items()}\n",
    "\n",
    "    def calculate_recall(self, true_pids, pred_indices):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices)\n",
    "        if len(true_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / len(true_set)\n",
    "\n",
    "    def calculate_precision_at_k(self, true_pids, pred_indices, k):\n",
    "        true_set = set(true_pids)\n",
    "        pred_set = set(pred_indices[:k])\n",
    "        if len(pred_set) == 0:\n",
    "            return 0\n",
    "        return len(true_set & pred_set) / k\n",
    "\n",
    "    def average_precision(self, true_pids, pred_indices):\n",
    "        relevant = 0\n",
    "        sum_precisions = 0\n",
    "        for i, pred in enumerate(pred_indices):\n",
    "            if pred in true_pids:\n",
    "                relevant += 1\n",
    "                sum_precisions += relevant / (i + 1)\n",
    "        if relevant == 0:\n",
    "            return 0\n",
    "        return sum_precisions / len(true_pids)\n",
    "\n",
    "    def mean_reciprocal_rank(self, true_pids, pred_indices):\n",
    "        for rank, pid in enumerate(pred_indices, start=1):\n",
    "            if pid in true_pids:\n",
    "                return 1 / rank\n",
    "        return 0\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        recalls = []\n",
    "        precisions_k = []\n",
    "        aps = []\n",
    "        mrrs = []\n",
    "\n",
    "        for query_id, true_ids in self.true_data.items():\n",
    "            pred_ids = self.predictions.get(query_id, [])\n",
    "            recalls.append(self.calculate_recall(true_ids, pred_ids))\n",
    "            precisions_k.append(self.calculate_precision_at_k(true_ids, pred_ids, 10))\n",
    "            aps.append(self.average_precision(true_ids, pred_ids))\n",
    "            mrrs.append(self.mean_reciprocal_rank(true_ids, pred_ids))\n",
    "\n",
    "        mean_recall = sum(recalls) / len(recalls)\n",
    "        mean_precision_at_k = sum(precisions_k) / len(precisions_k)\n",
    "        mean_ap = sum(aps) / len(aps)\n",
    "        mean_mrr = sum(mrrs) / len(mrrs)\n",
    "\n",
    "        print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "        print(f\"Precision@10: {mean_precision_at_k:.4f}\")\n",
    "        print(f\"Mean Average Precision: {mean_ap:.4f}\")\n",
    "        print(f\"Mean Reciprocal Rank: {mean_mrr:.4f}\")\n",
    "\n",
    "        return mean_recall, mean_precision_at_k, mean_ap, mean_mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating clinical trial metrics...\n",
      "Mean Recall: 1.0000\n",
      "Precision@10: 0.5000\n",
      "Mean Average Precision: 0.2250\n",
      "Mean Reciprocal Rank: 0.7137\n",
      "retrieved_docs_clinical saved as JSON.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating clinical trial metrics...\")\n",
    "evaluation_clinical = EvaluationMetrics(real_relevant_clinical, retrieved_docs_clinical)\n",
    "mean_recall, mean_precision_at_k, mean_ap, mean_mrr = evaluation_clinical.calculate_metrics()\n",
    "print(\"retrieved_docs_clinical saved as JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
